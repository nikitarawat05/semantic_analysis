# FastText Word Embeddings and STS Benchmark Evaluation

## Project Overview
This project demonstrates the use of FastText for training word embeddings and evaluating them on the STS Benchmark dataset. FastText is employed to train continuous bag of words (CBOW) models, which are then used to calculate word similarities and sentence embeddings. The STS Benchmark dataset is utilized to evaluate the performance of these embeddings against human-annotated similarity scores using Pearson correlation coefficient.

## Data Preparation
### Wikipedia Text Preprocessing
The Wikipedia dataset (`wikipedia:20220301.en`) is preprocessed to extract and clean text for training FastText models. Only the first 10,000 data points are used for training, and the preprocessed text is stored in `pt10000.txt`.

## Training FastText Model
FastText models are trained on the preprocessed text using CBOW with 100 dimensions. The trained models (`model10000.bin`) are then loaded for further evaluation.

## Word Embeddings and Similarity Calculation
Word vectors are extracted from the trained FastText models to calculate cosine similarities between selected word pairs (`king`, `man`, `woman`). Cosine similarity values are computed using NumPy.

## STS Benchmark Evaluation
The STS Benchmark dataset is used to evaluate the sentence embeddings generated by the FastText models. Sentence embeddings are calculated by averaging word embeddings. Cosine similarity is computed between sentence embeddings to compare against human-annotated similarity scores from the dataset.

## Running the Code
To reproduce the experiments or run the code:
- Ensure Python 3.x and pip are installed.
- Clone the repository and navigate to the project directory.
- Install dependencies using `pip install -r requirements.txt`.
- Download the model after training it from fasttext.py file then upload to a app.py file for processing or simulating the project.
- Run the main script or Jupyter notebook to train models, calculate embeddings, and evaluate using the STS Benchmark dataset.

## Files and Directory Structure
- `pt10000.txt`: Preprocessed text file containing Wikipedia data for training.
- `model10000.bin`: Trained FastText model file (CBOW, 100 dimensions).
- `README.md`: This file providing an overview of the project.
- `requirements.txt`: List of Python packages required for the project.

